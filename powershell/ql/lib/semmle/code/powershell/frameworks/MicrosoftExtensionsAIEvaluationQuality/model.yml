extensions:
  - addsTo:
      pack: microsoft/powershell-all
      extensible: typeModel
    data:
    - ["system.boolean", "microsoft.extensions.ai.evaluation.quality.chatconversationevaluator", "Member[ignoreshistory]"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.relevancetruthandcompletenessevaluator!", "Member[completenessmetricname]"]
    - ["system.threading.tasks.valuetask<system.string>", "microsoft.extensions.ai.evaluation.quality.relevancetruthandcompletenessevaluator", "Method[renderevaluationpromptasync].ReturnValue"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.equivalenceevaluator!", "Member[equivalencemetricname]"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.groundednessevaluator", "Member[metricname]"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.fluencyevaluator!", "Member[fluencymetricname]"]
    - ["system.collections.generic.ireadonlycollection<system.string>", "microsoft.extensions.ai.evaluation.quality.chatconversationevaluator", "Member[evaluationmetricnames]"]
    - ["microsoft.extensions.ai.evaluation.evaluationresult", "microsoft.extensions.ai.evaluation.quality.chatconversationevaluator", "Method[initializeresult].ReturnValue"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.singlenumericmetricevaluator", "Member[metricname]"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.relevancetruthandcompletenessevaluator!", "Member[truthmetricname]"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.coherenceevaluator!", "Member[coherencemetricname]"]
    - ["microsoft.extensions.ai.chatoptions", "microsoft.extensions.ai.evaluation.quality.relevancetruthandcompletenessevaluator", "Member[chatoptions]"]
    - ["system.collections.generic.ireadonlycollection<system.string>", "microsoft.extensions.ai.evaluation.quality.singlenumericmetricevaluator", "Member[evaluationmetricnames]"]
    - ["microsoft.extensions.ai.evaluation.evaluationresult", "microsoft.extensions.ai.evaluation.quality.relevancetruthandcompletenessevaluator", "Method[initializeresult].ReturnValue"]
    - ["system.boolean", "microsoft.extensions.ai.evaluation.quality.equivalenceevaluator", "Member[ignoreshistory]"]
    - ["microsoft.extensions.ai.chatoptions", "microsoft.extensions.ai.evaluation.quality.chatconversationevaluator", "Member[chatoptions]"]
    - ["system.threading.tasks.valuetask<system.string>", "microsoft.extensions.ai.evaluation.quality.groundednessevaluator", "Method[renderevaluationpromptasync].ReturnValue"]
    - ["system.threading.tasks.valuetask<system.string>", "microsoft.extensions.ai.evaluation.quality.chatconversationevaluator", "Method[renderasync].ReturnValue"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.singlenumericmetricevaluator", "Member[systemprompt]"]
    - ["system.threading.tasks.valuetask<system.string>", "microsoft.extensions.ai.evaluation.quality.chatconversationevaluator", "Method[renderevaluationpromptasync].ReturnValue"]
    - ["microsoft.extensions.ai.evaluation.evaluationresult", "microsoft.extensions.ai.evaluation.quality.singlenumericmetricevaluator", "Method[initializeresult].ReturnValue"]
    - ["system.boolean", "microsoft.extensions.ai.evaluation.quality.fluencyevaluator", "Member[ignoreshistory]"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.coherenceevaluator", "Member[metricname]"]
    - ["system.threading.tasks.valuetask", "microsoft.extensions.ai.evaluation.quality.chatconversationevaluator", "Method[parseevaluationresponseasync].ReturnValue"]
    - ["system.boolean", "microsoft.extensions.ai.evaluation.quality.groundednessevaluator", "Member[ignoreshistory]"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.groundednessevaluator!", "Member[groundednessmetricname]"]
    - ["system.threading.tasks.valuetask<system.string>", "microsoft.extensions.ai.evaluation.quality.coherenceevaluator", "Method[renderevaluationpromptasync].ReturnValue"]
    - ["system.collections.generic.ireadonlycollection<system.string>", "microsoft.extensions.ai.evaluation.quality.relevancetruthandcompletenessevaluator", "Member[evaluationmetricnames]"]
    - ["system.threading.tasks.valuetask<system.string>", "microsoft.extensions.ai.evaluation.quality.equivalenceevaluator", "Method[renderevaluationpromptasync].ReturnValue"]
    - ["system.boolean", "microsoft.extensions.ai.evaluation.quality.coherenceevaluator", "Member[ignoreshistory]"]
    - ["system.boolean", "microsoft.extensions.ai.evaluation.quality.relevancetruthandcompletenessevaluator", "Member[ignoreshistory]"]
    - ["system.threading.tasks.valuetask<system.string>", "microsoft.extensions.ai.evaluation.quality.fluencyevaluator", "Method[renderevaluationpromptasync].ReturnValue"]
    - ["system.threading.tasks.valuetask", "microsoft.extensions.ai.evaluation.quality.singlenumericmetricevaluator", "Method[parseevaluationresponseasync].ReturnValue"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.relevancetruthandcompletenessevaluator!", "Member[relevancemetricname]"]
    - ["system.threading.tasks.valuetask<system.boolean>", "microsoft.extensions.ai.evaluation.quality.chatconversationevaluator", "Method[canrenderasync].ReturnValue"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.fluencyevaluator", "Member[metricname]"]
    - ["system.threading.tasks.valuetask<microsoft.extensions.ai.evaluation.evaluationresult>", "microsoft.extensions.ai.evaluation.quality.chatconversationevaluator", "Method[evaluateasync].ReturnValue"]
    - ["microsoft.extensions.ai.chatoptions", "microsoft.extensions.ai.evaluation.quality.singlenumericmetricevaluator", "Member[chatoptions]"]
    - ["system.threading.tasks.valuetask", "microsoft.extensions.ai.evaluation.quality.relevancetruthandcompletenessevaluator", "Method[parseevaluationresponseasync].ReturnValue"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.chatconversationevaluator", "Member[systemprompt]"]
    - ["system.string", "microsoft.extensions.ai.evaluation.quality.equivalenceevaluator", "Member[metricname]"]
